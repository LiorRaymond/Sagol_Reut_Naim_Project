#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Title: Logistic Regression Example
Author: ZoÃ« E. Laky, M.A.
Contact: zoe.laky@nih.gov
Date: 07MAY2025

Dataset(s):
- example datasets

Versions: 
- Written for Python 3.10.15, Spyder 6.0.3

Notes:
    
"""
#importa packages
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import LeaveOneOut
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import balanced_accuracy_score, accuracy_score, classification_report

#example data
df = pd.DataFrame({
    'ID':         ['P01', 'P02', 'P03', 'P04', 'P05', 'P06', 'P07', 'P08'],
    'feature_1':  [0.1,   1.2,   1.4,   0.3,   0.6,   0.8,   0.8,   0.7],
    'feature_2':  [1.1,   0.9,   1.3,   0.7,   1.5,   1.2,   0.5,   1.5],
    'target':     [0,     1,     1,     0,     1,     0,     1,     1]
})

#separate features and dependent
X = df[['feature_1', 'feature_2']]
y = df['target']
ids = df['ID']

#initialize LOOCV and LogReg
loo = LeaveOneOut()
model = LogisticRegression(solver='liblinear', class_weight='balanced')

#initialize results list
results = []

for train_index, test_index in loo.split(X):
    #split data 
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    test_id = ids.iloc[test_index].values[0]
    
    scaler = StandardScaler() #make sure you scale the data within the loop to prevent data leakage
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    #train and predict
    model.fit(X_train_scaled, y_train)
    pred = model.predict(X_test_scaled)

    #save results
    results.append({
        'test_id': test_id,
        'y_true': y_test.values[0],
        'y_pred': pred[0]
    })

#convert results list to a dataframe
results_df = pd.DataFrame(results)
print("LOOCV Results:")
print(results_df)

#overall accuracy
overall_accuracy = accuracy_score(results_df['y_true'], results_df['y_pred'])
print("\nOverall Accuracy:", overall_accuracy)

#balanced accuracy
balanced_accuracy = balanced_accuracy_score(results_df['y_true'], results_df['y_pred'])
print("\nBalanced Accuracy:", balanced_accuracy)

#zero division error handling included
print("\nClassification Report:")
print(classification_report(results_df['y_true'], results_df['y_pred'], zero_division=0))

#manual per-class accuracy: accuracy for each class based on exact matches between y_true and y_pred
class_accuracy = (
    results_df.groupby('y_true')[['y_true', 'y_pred']]
    .apply(lambda g: (g['y_true'] == g['y_pred']).mean())
    .to_dict()
)
print("\nManual Per-class Accuracy:", class_accuracy)

#manual calcualtion of balanaced accuracy: average of recalls (true positive rates) for each class
recalls = {}
classes = results_df['y_true'].unique()

for cls in classes:
    true_mask = results_df['y_true'] == cls #only ids for class of interest (DV=1 OR DV=0)
    correct_preds = (results_df['y_pred'][true_mask] == cls).sum() #number of correct predictions for class of interest
    total_true = true_mask.sum() #total true ids
    #recall = true positives / total actual positives
    recalls[cls] = correct_preds / total_true if total_true > 0 else 0.0

#balanced accuracy = average of recalls for each class
balanced_accuracy_manual = sum(recalls.values()) / len(recalls)
